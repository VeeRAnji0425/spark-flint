My rough notes for things that I will like to do and report. It is of no use to anyone else, but is checked in as it is my repo :).

Tutorial
1. Spark Shell
2. RDD collection
3. Distribute


Issues and Features
1. Reserved keywords in schemaRDD for example
case class CommitHistory(name:String, count: Int)
runQuery("select name, count from commitHistory")


Why Spark
1. Scala makes it so good, sortBy, map, reduceByKey. Functions as first class citizens. "_", tuples.
2. Implicit


Questions
1. How do I see the table def


Handy Commands
def gis(s: String) = new GZIPInputStream(new BufferedInputStream(new FileInpuuthtStream(s)))
def contents = Source.fromInputStream(gis("github/data/2013-04-11-9.json.gz"))

val locs = sqlContext.sql("select locs from commits").collect.map(row=> row(0).asInstanceOf[Map[String,Int]])
case class Churn(fileName:String)
val locsRDD = sc.parallelize(locs.flatMap(loc => loc.keys).map{new Churn(_)})
locsRDD.registerAsTable("churnsAlt")
locsRDD.cache

local-cluster[1,1,512]

Anti Thoughts
"One example is that it is dependent on available memory; any large dataset that exceeds that will hit a huge performance wall." [http://www.informationweek.com/big-data/big-data-analytics/will-spark-google-dataflow-steal-hadoops-thunder/a/d-id/1278959?page_number=2]


ADD_JARS="../lib/jcommon-1.0.22.jar,../lib/jfreechart-1.0.18.jar"

java.util.Date is not handled
val fos = new FileOutputStream("/data/work/projects/DataEngineering/work/captured.tmp")
fos.write(res1)
fos.close



Important files
./sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/SqlParser.scala

curl 'http://localhost:4040/plugins/command.json' -H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' --data 'payload={"command":"desc","args":"activities"}'


../../spark/bin/spark-shell --jars /home/apurba/.m2/repository/org/ow2/asm/asm-all/5.0.3/asm-all-5.0.3.jar --total-executor-cores 4 --master local
