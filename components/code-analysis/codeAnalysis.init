import java.io._
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkContext
import java.nio.file.{Paths, Files}

:load ./components/code-analysis/GitLogParser.scala
:load ./components/code-analysis/LocalFPGrowth.scala

case class Churn(fileName:String)
case class FPair(fileGroup:String, groupSize:Int)

// creates a RDD from the passed file
def commitLogToRdd(fileName:String):org.apache.spark.rdd.RDD[Commit] = {
  // read the commit log into a simple collection
  val commitIterator = GitLogProcessor.iterate(new BufferedReader(new InputStreamReader(new FileInputStream(fileName))))
  val commitArray = commitIterator.toArray

  // transform so that spark can handle the locs (file vs changed lines count) map
  val serializableCommitArray = commitArray.map{commit=> Commit(commit.hash, commit.author, commit.timestamp, commit.year, commit.month, commit.dayOfWeek, commit.message, commit.metrics.map(identity))}
  val commitRDD = sc.parallelize(serializableCommitArray)
  commitRDD.cache
  return commitRDD
}

// utility method to create a table from the commit log
def registerCommitsAsTable(filePath:String, tableName:String) = {
  val commitRDD = commitLogToRdd(filePath)
  commitRDD.registerAsTable(tableName)
  commitRDD.cache
}

// utility method to create chruns table (mines file names from the commits) from the commits table
def registerChurnsAsTable(commitTableName:String, tableName:String = "churns") = {
  val fileGroups = sqlContext.sql(s"select metrics from $commitTableName").collect.map(row=> row(0).asInstanceOf[Map[String,Int]])
  val fileGroupsRDD = sc.parallelize(fileGroups.flatMap(fileGroup => fileGroup.keys).map{new Churn(_)})
  fileGroupsRDD.cache
  fileGroupsRDD.registerAsTable(tableName)
}

// FP-Growth to identify groups of files that are modified together, operates on commits and churns table. We can pass minSupport (number of times the file must appear in overall churns),  min + stdDev is a good start for this
def computeFrequentItems(commitTableName:String, churnTableName:String, tableName:String = "fpairs", minSupport:Long = 3L) {
  // get the list of files in each commit
  val lofs = sqlContext.sql(s"select metrics from $commitTableName").map(row => row(0).asInstanceOf[Map[String,Int]])

  // get churn (file vs change count) as a map
  val fList = sqlContext.sql(s"select fileName, count(*) from $churnTableName group by fileName").map(row => (row(0).asInstanceOf[String],row(1).asInstanceOf[Long])).collect.foldLeft(Map():Map[String,Long])((m,v) => m + v)

  // create a RDD from file list where each file group in each commit is sorted by its overall count across all commits
  val lofListRDD = lofs.map(lof => lof.keys.toList.filter{fList(_) > minSupport}.sortWith{ fList(_) > fList(_)}.fold("":String){(m,v) => m+" " +v}.trim ).filter(lof => lof.count(_ == ' ') > 0)

  // filter out all files that are not changed much (less than minimum support)
  val fItems = fList.filter(_._2 > minSupport).map(_._1)
  val localLofs = lofListRDD.collect

  // create a FP-Tree from it
  val root = new Node("root")
  localLofs.foreach{lof => root.addChildren(lof.split(" "))}
  val fPairs = fItems.foldLeft(List():List[String])((a, v) => a ++ FPGrowthOps.projections(minSupport.toInt - 1 , v, root)).distinct.sorted

  // map the tree as a RDD 
  val fPairRDD = sc.parallelize(fPairs.map(fPair => FPair(fPair, fPair.count(_ == ' ') + 1)))

  // register this list of groups of files that are changed together as a table
  fPairRDD.registerAsTable(tableName)
}

// sample run in case there is a file at this location
val filePath = "./data/detailed.spark.git.log"
if (Files.exists(Paths.get(filePath))) {
  registerCommitsAsTable(filePath, "commits")
  registerChurnsAsTable("commits")
  computeFrequentItems("commits", "churns")
}

// register it with the component manager for help in case anyone is interested later
ComponentManager.register("code-analysis", """ Code Analysis Component
Adds  registerCommitsAsTable
      registerChurnsAsTable
      computeFrequentItems
Methods, for details look at codeAnalysis.init""")
ComponentManager.describe("code-analysis")

